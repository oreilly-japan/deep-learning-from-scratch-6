from collections import defaultdict

def count_pairs(ids):
    counts = defaultdict(int)
    for pair in zip(ids, ids[1:]):
        counts[pair] += 1
    return counts

def merge(ids, pair, new_id):
    merged_ids = []
    i = 0

    while i < len(ids):
        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:
            merged_ids.append(new_id)
            i += 2
        else:
            merged_ids.append(ids[i])
            i += 1

    return merged_ids


class BPETokenizer:
    def __init__(self, merge_rules):
        self.merge_rules = merge_rules

        # IDã‹ã‚‰ãƒã‚¤ãƒˆåˆ—ã¸ã®å¯¾å¿œè¡¨ï¼ˆ0~255ã‚’ç™»éŒ²ï¼‰
        self.id_to_byte = {i: bytes([i]) for i in range(256)}

        # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒã‚¤ãƒˆåˆ—ã‚’é€£çµ
        for (token1, token2), new_id in merge_rules.items():
            self.id_to_byte[new_id] = self.id_to_byte[token1] + self.id_to_byte[token2]

        # èªå½™ã‚µã‚¤ã‚ºã‚’è¨­å®š
        self.vocab_size = len(self.id_to_byte)

    def encode(self, text):
        ids = list(text.encode("utf-8"))

        # å­¦ç¿’æ™‚ã®é †åºã§ãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
        for merge_pair, new_id in self.merge_rules.items():
            ids = merge(ids, merge_pair, new_id)

        return ids

    def decode(self, ids):
        # å„ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å¯¾å¿œã™ã‚‹ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
        byte_list = [self.id_to_byte[i] for i in ids]

        # ã™ã¹ã¦ã®ãƒã‚¤ãƒˆåˆ—ã‚’é€£çµ
        combined_bytes = b"".join(byte_list)

        # ãƒã‚¤ãƒˆåˆ—ã‚’UTF-8ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        text = combined_bytes.decode("utf-8", errors="replace")
        return text

# å­¦ç¿’æ¸ˆã¿ã®ãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«
merge_rules = {(105, 115): 256, (256, 32): 257, (105, 110): 258, (72, 101): 259}

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
tokenizer = BPETokenizer(merge_rules)

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
text = "Helloä¸–ç•ŒğŸ˜"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)

print(ids)  # [259, 108, 108, 111, 228, 184, 150, 231, 149, 140, 240, 159, 152, 129]
print(decoded)  # Helloä¸–ç•ŒğŸ˜