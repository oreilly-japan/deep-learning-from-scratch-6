import os
import sys
os.chdir(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
sys.path.append('.')

import json
import torch
import torch.nn.functional as F
from openai import OpenAI
from storybot.model import GPT
from storybot.tokenizer import BPETokenizer
from storybot.utils import get_device, generate

# è¨­å®š
# ==========================================
client = OpenAI(api_key="your_api_key_here")
# ==========================================
device = get_device()
tokenizer_path = 'storybot/merge_rules.pkl'
tokenizer = BPETokenizer.load_from(tokenizer_path)

# æ¯”è¼ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«
model_paths = {
    'pretrain': 'storybot/model_pretrain.pt',
    'dpo': 'storybot/model_dpo.pt',
}

# è©•ä¾¡è¨­å®š
prompt = "Once upon a time"
num_comparisons = 100  # æ¯”è¼ƒå›æ•°
max_new_tokens = 150
temperature = 1.0


def compare_stories(client, story_a, story_b):
    """2ã¤ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ¯”è¼ƒã—ã€ã©ã¡ã‚‰ãŒã‚ˆã‚Šãƒãƒƒãƒ”ãƒ¼ã‚¨ãƒ³ãƒ‰ã‹ã‚’åˆ¤å®š"""

    evaluation_prompt = f"""ä»¥ä¸‹ã®2ã¤ã®å­ä¾›å‘ã‘ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ¯”è¼ƒã—ã€ã©ã¡ã‚‰ãŒã‚ˆã‚Šãƒãƒƒãƒ”ãƒ¼ã‚¨ãƒ³ãƒ‰ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€Story Aã€‘
{story_a}

ã€Story Bã€‘
{story_b}

ã©ã¡ã‚‰ãŒã‚ˆã‚Šæ˜ã‚‹ãå¹¸ã›ãªçµæœ«ã‹ã€ã¾ãŸã¯å¸Œæœ›ã«æº€ã¡ãŸå†…å®¹ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
JSONå½¢å¼ã§å›ç­”: {{"winner": "A" or "B" or "tie", "reason": "ç°¡æ½”ãªç†ç”±"}}"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": evaluation_prompt}],
        max_tokens=150,
        response_format={"type": "json_object"}
    )

    text = response.choices[0].message.content
    return json.loads(text)

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model_pretrain = GPT.load_from(model_paths['pretrain'], device=device)
model_dpo = GPT.load_from(model_paths['dpo'], device=device)

# çµæœã‚’è¨˜éŒ²
results = []
wins = {"pretrain": 0, "dpo": 0, "tie": 0}

for i in range(num_comparisons):
    print(f"\n{'='*60}")
    print(f"Comparison {i+1}/{num_comparisons}")
    print('='*60)

    # ä¸¡ãƒ¢ãƒ‡ãƒ«ã§ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆ
    story_pretrain = generate(model_pretrain, tokenizer, prompt, max_new_tokens, temperature)
    story_dpo = generate(model_dpo, tokenizer, prompt, max_new_tokens, temperature)

    print(f"\n[Pretrain]: {story_pretrain[:100]}...")
    print(f"\n[DPO]: {story_dpo[:100]}...")

    # ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã‚’é¿ã‘ã‚‹ãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ ã«é †åºã‚’å…¥ã‚Œæ›¿ãˆ
    import random
    if random.random() < 0.5:
        story_a, story_b = story_pretrain, story_dpo
        mapping = {"A": "pretrain", "B": "dpo"}
    else:
        story_a, story_b = story_dpo, story_pretrain
        mapping = {"A": "dpo", "B": "pretrain"}

    # LLM-as-a-Judgeã§æ¯”è¼ƒ
    judgment = compare_stories(client, story_a, story_b)

    winner_label = judgment["winner"]
    if winner_label == "tie":
        winner = "tie"
    else:
        winner = mapping[winner_label]

    wins[winner] += 1

    print(f"\nğŸ† Winner: {winner}")
    print(f"   Reason: {judgment['reason']}")

    results.append({
        "story_pretrain": story_pretrain,
        "story_dpo": story_dpo,
        "winner": winner,
        "reason": judgment["reason"]
    })

# ã‚µãƒãƒªãƒ¼å‡ºåŠ›
print("\n" + "="*60)
print("ğŸ“Š PAIRWISE COMPARISON RESULTS")
print("="*60)

total = num_comparisons
print(f"\n  Pretrain wins: {wins['pretrain']:3d} ({wins['pretrain']/total*100:5.1f}%)")
print(f"  DPO wins:      {wins['dpo']:3d} ({wins['dpo']/total*100:5.1f}%)")
print(f"  Ties:          {wins['tie']:3d} ({wins['tie']/total*100:5.1f}%)")

# å‹ç‡ï¼ˆtieã‚’é™¤ãï¼‰
if wins['pretrain'] + wins['dpo'] > 0:
    dpo_winrate = wins['dpo'] / (wins['pretrain'] + wins['dpo']) * 100
    print(f"\n  DPO win rate (excluding ties): {dpo_winrate:.1f}%")